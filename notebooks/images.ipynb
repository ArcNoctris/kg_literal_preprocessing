{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\00_CODE\\03_Master_Thesis\\rdf-literal-preprocessing\\src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ../src\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from utils import IMAGE_TYPES, RDF_DATE_TYPES, RDF_NUMBER_TYPES, POTENTIAL_TEXT_TYPES, RDF_DATE_TYPES, get_relevant_relations\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import VGG_image_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset):\n",
    "    with open(f\"../data/raw/{dataset}_final_torch_None.pickle\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    print(\"# nodes\")\n",
    "    print(len(data.i2e))\n",
    "    print(\"datatypes\")\n",
    "    print(data.datatypes())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes\n",
      "1153221\n",
      "datatypes\n",
      "['iri', 'blank_node', 'none', 'http://kgbench.info/dt#base64Image', 'http://www.w3.org/2001/XMLSchema#date', 'http://www.w3.org/2001/XMLSchema#decimal', 'http://www.w3.org/2001/XMLSchema#positiveInteger']\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('amplus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        921\n",
      "1        425\n",
      "2        921\n",
      "3        666\n",
      "4        117\n",
      "        ... \n",
      "58850    633\n",
      "58851    551\n",
      "58852    883\n",
      "58853    619\n",
      "58854    714\n",
      "Name: class, Length: 58855, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = VGG_image_classification(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .util import here, tic, toc, getfile\n",
    "# from .urls import URLS\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import gzip, base64, io, sys, warnings, wget, os, random\n",
    "\n",
    "import torch\n",
    "from utils import get_relevant_relations, IMAGE_TYPES\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "URI_PREFIX =\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(b64):\n",
    "    try:\n",
    "        imgdata = base64.urlsafe_b64decode(b64)\n",
    "    except:\n",
    "        # print(f'Could not decode b64 string {b64}')\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(b64):\n",
    "    try:\n",
    "        imgdata = base64.urlsafe_b64decode(b64)\n",
    "    except:\n",
    "        print(f'Could not decode b64 string {b64}')\n",
    "        return Image.new('RGB', (1, 1))\n",
    "\n",
    "\n",
    "    try:\n",
    "        return Image.open(io.BytesIO(base64.urlsafe_b64decode(b64)))\n",
    "    except:\n",
    "        return Image.new('RGB', (1, 1))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "relevant_relations = get_relevant_relations(data, IMAGE_TYPES)\n",
    "df = pd.DataFrame(data.triples[torch.isin(data.triples[:,1],torch.tensor(relevant_relations))], columns = [\"s\",\"p\",\"o\"])\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg = vgg.to(device)\n",
    "vgg.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Resize(256),  # Resize the image to 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Crop the center 224x224 pixels\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_image(image, preprocess, model, device= \"cpu\"):\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    input_batch = input_batch.to(device)\n",
    "    return vgg(input_batch).squeeze(0).softmax(0).argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1341602400.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"b64\"] = df['o'].apply(lambda x: data.i2e[x][0])\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1341602400.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"is_image\"] = df[\"b64\"].apply(lambda x: is_image(x))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df[\"b64\"] = df['o'].apply(lambda x: data.i2e[x][0])\n",
    "df[\"is_image\"] = df[\"b64\"].apply(lambda x: is_image(x))\n",
    "df = df[df['is_image']==True]\n",
    "df[\"image\"] = df['b64'].apply(lambda x: get_image(x))\n",
    "df['class']= df[\"image\"].apply(lambda x: eval_image(x, preprocess=preprocess, model= vgg, device= device) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
      "C:\\Users\\Noctris\\AppData\\Local\\Temp\\ipykernel_16728\\1004141515.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n"
     ]
    }
   ],
   "source": [
    "for pred in df['p'].unique(): # use this over relation since loading image can subset df\n",
    "    sub_df = df[df['p']==pred]\n",
    "    p = f'{URI_PREFIX}predicat#img-class-{pred}'\n",
    "    new_id = len(data.i2r)\n",
    "    data.r2i[p] = new_id\n",
    "    data.i2r.append(p)\n",
    "    data.num_relations += 1\n",
    "    for c in sub_df[\"class\"].unique():\n",
    "        o = (f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
    "        f'{URI_PREFIX}datatype#img-class')\n",
    "        new_id = len(data.i2e)\n",
    "        data.e2i[o] = new_id\n",
    "        data.i2e.append(o)\n",
    "        data.num_entities += 1\n",
    "\n",
    "        \n",
    "    sub_df['new_o'] = sub_df[\"class\"].apply(lambda c: data.e2i[(f'{URI_PREFIX}entity#img-class-{c}-{pred}',\n",
    "        f'{URI_PREFIX}datatype#img-class')])\n",
    "    sub_df['new_p'] = sub_df[\"class\"].apply(lambda f: data.r2i[f'{URI_PREFIX}predicat#img-class-{pred}'])\n",
    "    #print(df)\n",
    "    ten = torch.tensor(sub_df[['s','new_p','new_o']].values.astype(np.int32), dtype= torch.int32)\n",
    "    data.triples = torch.cat((data.triples, ten), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[185557,    131, 127327],\n",
       "        [182860,     25, 119738],\n",
       "        [207157,     45, 293297],\n",
       "        ...,\n",
       "        [188968,    160, 350676],\n",
       "        [204311,    160, 350676],\n",
       "        [230999,    160, 350676]], dtype=torch.int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('testentity#img-class-644-9', 'testdatatype#img-class')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.i2e[350676]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68432"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68432"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=42x64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"image\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_image(df[\"image\"].iloc[3], preprocess=preprocess, model= vgg, device= device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>p</th>\n",
       "      <th>o</th>\n",
       "      <th>b64</th>\n",
       "      <th>image</th>\n",
       "      <th>is_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207157</td>\n",
       "      <td>45</td>\n",
       "      <td>293297</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAACoAAABACAIAAADjznIwAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>257290</td>\n",
       "      <td>29</td>\n",
       "      <td>86418</td>\n",
       "      <td>http://commons.wikimedia.org/wiki/Special:File...</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=RGB size=1x1 at 0x...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229304</td>\n",
       "      <td>45</td>\n",
       "      <td>304193</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAACsAAABACAIAAAAMDBkOAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120986</td>\n",
       "      <td>45</td>\n",
       "      <td>326434</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAADkAAABACAIAAAAvV0jbAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170295</td>\n",
       "      <td>45</td>\n",
       "      <td>299580</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAACsAAABACAIAAAAMDBkOAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70180</th>\n",
       "      <td>213627</td>\n",
       "      <td>45</td>\n",
       "      <td>337042</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAEAAAAArCAIAAABHOBkQAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70181</th>\n",
       "      <td>107134</td>\n",
       "      <td>45</td>\n",
       "      <td>286640</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAC8AAABACAIAAAAF57l0AA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70182</th>\n",
       "      <td>166861</td>\n",
       "      <td>45</td>\n",
       "      <td>283867</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAC4AAABACAIAAADqJdJKAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70183</th>\n",
       "      <td>246237</td>\n",
       "      <td>45</td>\n",
       "      <td>330935</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAAEAAAAAiCAIAAABgN0jYAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70184</th>\n",
       "      <td>169689</td>\n",
       "      <td>45</td>\n",
       "      <td>289195</td>\n",
       "      <td>iVBORw0KGgoAAAANSUhEUgAAACkAAABACAIAAAAI+ckzAA...</td>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68432 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            s   p       o                                                b64  \\\n",
       "0      207157  45  293297  iVBORw0KGgoAAAANSUhEUgAAACoAAABACAIAAADjznIwAA...   \n",
       "1      257290  29   86418  http://commons.wikimedia.org/wiki/Special:File...   \n",
       "2      229304  45  304193  iVBORw0KGgoAAAANSUhEUgAAACsAAABACAIAAAAMDBkOAA...   \n",
       "3      120986  45  326434  iVBORw0KGgoAAAANSUhEUgAAADkAAABACAIAAAAvV0jbAA...   \n",
       "4      170295  45  299580  iVBORw0KGgoAAAANSUhEUgAAACsAAABACAIAAAAMDBkOAA...   \n",
       "...       ...  ..     ...                                                ...   \n",
       "70180  213627  45  337042  iVBORw0KGgoAAAANSUhEUgAAAEAAAAArCAIAAABHOBkQAA...   \n",
       "70181  107134  45  286640  iVBORw0KGgoAAAANSUhEUgAAAC8AAABACAIAAAAF57l0AA...   \n",
       "70182  166861  45  283867  iVBORw0KGgoAAAANSUhEUgAAAC4AAABACAIAAADqJdJKAA...   \n",
       "70183  246237  45  330935  iVBORw0KGgoAAAANSUhEUgAAAEAAAAAiCAIAAABgN0jYAA...   \n",
       "70184  169689  45  289195  iVBORw0KGgoAAAANSUhEUgAAACkAAABACAIAAAAI+ckzAA...   \n",
       "\n",
       "                                                   image  is_image  \n",
       "0      <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "1      <PIL.Image.Image image mode=RGB size=1x1 at 0x...      True  \n",
       "2      <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "3      <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "4      <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "...                                                  ...       ...  \n",
       "70180  <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "70181  <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "70182  <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "70183  <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "70184  <PIL.PngImagePlugin.PngImageFile image mode=RG...      True  \n",
       "\n",
       "[68432 rows x 6 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in df:\n",
    "    b64 = data.i2e[triple[2]][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.i2e[df[0,2]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# res:Image\n",
    "# Take in base64 string and return cv image\n",
    "num_noparse = 0\n",
    "b64 = data.i2e[df[0,2]][0]\n",
    "\n",
    "try:\n",
    "    imgdata = base64.urlsafe_b64decode(b64)\n",
    "except:\n",
    "    print(f'Could not decode b64 string {b64}')\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    res = Image.open(io.BytesIO(imgdata))\n",
    "except:\n",
    "    res = Image.new('RGB', (1, 1))\n",
    "    # num_noparse += 1\n",
    "    # res.append(Image.new('RGB', (1, 1)))\n",
    "        # -- If the image can't be parsed, we insert a 1x1 black image\n",
    "\n",
    "# if num_noparse > 0:\n",
    "#     warnings.warn(f'There were {num_noparse} images that couldn\\'t be parsed. These have been replaced by black images.')\n",
    "\n",
    "# # print(num_noparse, 'unparseable', len([r for r in res if r is not None]), 'parseable')\n",
    "\n",
    "# return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=42x64>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data.get_images()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "import torchvision.transforms as transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Resize(256),  # Resize the image to 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Crop the center 224x224 pixels\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = input_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = vgg(input_batch).squeeze(0).softmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_batch = input_batch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x49 and 25088x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vgg(input_tensor )\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\vgg.py:69\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[0;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x49 and 25088x4096)"
     ]
    }
   ],
   "source": [
    "vgg(input_tensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Noctris/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:06<00:00, 83.6MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained VGG model\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vgg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (PngImageFile, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!PngImageFile!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!PngImageFile!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vgg(img)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torchvision\\models\\vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (PngImageFile, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!PngImageFile!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!PngImageFile!, !Parameter!, !Parameter!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "vgg(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdf-literal-preprocessing-20b3_M0v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
