{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "import dataload\n",
    "import embed\n",
    "import evaluate\n",
    "from utils.data_utils import data_to_kg, extract_ents\n",
    "import preprocess\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import fire\n",
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kgbench as kg\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as pl\n",
    "import random as rd\n",
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import RDF_NUMBER_TYPES, get_relevant_relations, add_triple, get_p_types\n",
    "from kgbench.load import Data\n",
    "from typing import List, Sequence, Tuple\n",
    "\n",
    "from kgbench import load, tic, toc, d\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from utils import URI_PREFIX\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from kgbench import load\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data mdgenre (80.59s).\n",
      "pruned (14.33s).\n"
     ]
    }
   ],
   "source": [
    "data = kg.load(name=\"mdgenre\", torch=True, final=True, prune_dist=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_df = data.dgl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_mask': tensor([False, False, False,  ..., False, False, False]), 'withheld_mask': tensor([False, False, False,  ..., False, False, False]), 'label': tensor([-1, -1, -1,  ..., -1, -1, -1])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['training_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331089"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(g.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mdgenre: 154 relations, 12 classes.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoaded \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39mmdgenre\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mnum_rels\u001b[39m}\u001b[39;00m\u001b[39m relations, \u001b[39m\u001b[39m{\u001b[39;00mnum_classes\u001b[39m}\u001b[39;00m\u001b[39m classes.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m category \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mpredict_category\n\u001b[1;32m---> 13\u001b[0m labels \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49mnodes[category]\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mpop(\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# note: not labels\u001b[39;00m\n\u001b[0;32m     15\u001b[0m training_mask \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mnodes[category]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mtraining_mask\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m withheld_mask \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mnodes[category]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mwithheld_mask\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\_collections_abc.py:795\u001b[0m, in \u001b[0;36mMutableMapping.pop\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[39m  If key is not found, d is returned if given, otherwise KeyError is raised.\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    794\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 795\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[key]\n\u001b[0;32m    796\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m     \u001b[39mif\u001b[39;00m default \u001b[39mis\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__marker:\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\dgl\\view.py:73\u001b[0m, in \u001b[0;36mHeteroNodeDataView.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[0;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49m_get_n_repr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ntid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nodes)[key]\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\dgl\\frame.py:622\u001b[0m, in \u001b[0;36mFrame.__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    610\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the column of the given name.\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \n\u001b[0;32m    612\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[39m        Column data.\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_columns[name]\u001b[39m.\u001b[39mdata\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "#dataset = kg.load(name=\"mdgenre\", torch=True, final=True, prune_dist=2 ).dgl(to32=True)\n",
    "\n",
    "g = dataset[0].int()\n",
    "\n",
    "\n",
    "\n",
    "num_rels = len(g.canonical_etypes)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "print(f'Loaded {\"mdgenre\"}: {num_rels} relations, {num_classes} classes.')\n",
    "\n",
    "category = dataset.predict_category\n",
    "labels = g.nodes[category].data.pop('label') # note: not labels\n",
    "\n",
    "training_mask = g.nodes[category].data.pop('training_mask')\n",
    "withheld_mask = g.nodes[category].data.pop('withheld_mask')\n",
    "    # -- note the use of training/withheld rather than train/test\n",
    "\n",
    "for cetype in g.canonical_etypes:\n",
    "    g.edges[cetype].data['norm'] = dgl.norm_by_dst(g, cetype).unsqueeze(1) * 0.0 + 1.0\n",
    "\n",
    "g = dgl.to_homogeneous(g, edata=['norm'])\n",
    "\n",
    "training_idx = training_mask.nonzero()\n",
    "withheld_idx = withheld_mask.nonzero()\n",
    "    # -- nonzero() produces the indices of all the elements that are true\n",
    "\n",
    "training_labels = labels[training_idx].squeeze()\n",
    "withheld_labels = labels[withheld_idx].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resource'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import TransR,TransE\n",
    "model = TransR(num_rels=num_rels, rfeats=1,nfeats=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([176413, 173816, 197157,  ..., 180476, 198138, 158250],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.triples[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6076658847744 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mforward(data\u001b[39m.\u001b[39;49mtriples[:,\u001b[39m0\u001b[39;49m],data\u001b[39m.\u001b[39;49mtriples[:,\u001b[39m2\u001b[39;49m],data\u001b[39m.\u001b[39;49mtriples[:,\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\dgl\\nn\\pytorch\\link\\transr.py:103\u001b[0m, in \u001b[0;36mTransR.forward\u001b[1;34m(self, h_head, h_tail, rels)\u001b[0m\n\u001b[0;32m    101\u001b[0m h_rel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrel_emb(rels)\n\u001b[0;32m    102\u001b[0m proj_rel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrel_project(rels)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfeats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrfeats)\n\u001b[1;32m--> 103\u001b[0m h_head \u001b[39m=\u001b[39m (h_head\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m) \u001b[39m@\u001b[39;49m proj_rel)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    104\u001b[0m h_tail \u001b[39m=\u001b[39m (h_tail\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m@\u001b[39m proj_rel)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mnorm(h_head \u001b[39m+\u001b[39m h_rel \u001b[39m-\u001b[39m h_tail, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6076658847744 bytes."
     ]
    }
   ],
   "source": [
    "model.forward(data.triples[:,0],data.triples[:,2],data.triples[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGCN(num_nodes=g.num_nodes(), in_dim=256, h_dim=16, out_dim=dataset.num_classes, num_rels=num_rels, num_bases=40, reg='basis')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    logits = model(g)[training_idx].squeeze(1)\n",
    "    loss = F.cross_entropy(logits, training_labels.to(torch.long))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = (logits.argmax(dim=1) == training_labels).sum() / training_labels.size(0)\n",
    "    print(f'Epoch {epoch:05d} | Loss {loss.item():.4f} | Train Accuracy {acc:.4f} ')\n",
    "\n",
    "    acc = evaluate(g, withheld_idx, withheld_labels, model)\n",
    "    print(\"         Test accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'num_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdgl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m TransR,TransE\n\u001b[1;32m----> 2\u001b[0m scorer \u001b[39m=\u001b[39m TransE(num_nodes\u001b[39m=\u001b[39;49mg\u001b[39m.\u001b[39;49mnum_nodes(), in_dim\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, h_dim\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, out_dim\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mnum_classes, num_rels\u001b[39m=\u001b[39;49mnum_rels, num_bases\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m, reg\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbasis\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_nodes'"
     ]
    }
   ],
   "source": [
    "from dgl.nn import TransR,TransE\n",
    "scorer = TransE(num_rels=num_rels, feats=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from dgl.nn.pytorch import RelGraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    NB: This is an _embedding_ RGCN. It's slightly different from the classic RGCN in rgcn.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, in_dim, h_dim, out_dim, num_rels, num_bases, reg):\n",
    "        reg = None if reg == 'none' else reg\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(num_nodes, in_dim)\n",
    "        # nn.init.kaiming_normal_(self.emb.weight, mode='fan_in')\n",
    "\n",
    "        # two-layer RGCN\n",
    "        self.conv1 = RelGraphConv(in_dim, h_dim, num_rels, regularizer=reg, num_bases=num_bases, self_loop=False)\n",
    "        self.conv2 = RelGraphConv(h_dim, out_dim, num_rels, regularizer=reg, num_bases=num_bases, self_loop=False)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        x = self.emb.weight\n",
    "\n",
    "        h = F.relu(self.conv1(g, x, g.edata[dgl.ETYPE], g.edata['norm']))\n",
    "\n",
    "        h = self.conv2(g, h, g.edata[dgl.ETYPE], g.edata['norm'])\n",
    "\n",
    "        return h\n",
    "\n",
    "def evaluate(g, withheld_idx, labels, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g)[withheld_idx].squeeze(1)\n",
    "\n",
    "    acc = (logits.argmax(dim=1) == labels).sum() / labels.size(0)\n",
    "\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 2.4662 | Train Accuracy 0.0280 \n",
      "         Test accuracy 0.5410\n",
      "Epoch 00001 | Loss 2.3675 | Train Accuracy 0.5449 \n",
      "         Test accuracy 0.2080\n",
      "Epoch 00002 | Loss 2.3304 | Train Accuracy 0.2178 \n",
      "         Test accuracy 0.2093\n",
      "Epoch 00003 | Loss 2.2975 | Train Accuracy 0.2221 \n",
      "         Test accuracy 0.2097\n",
      "Epoch 00004 | Loss 2.2796 | Train Accuracy 0.2219 \n",
      "         Test accuracy 0.2093\n",
      "Epoch 00005 | Loss 2.2679 | Train Accuracy 0.2241 \n",
      "         Test accuracy 0.2070\n",
      "Epoch 00006 | Loss 2.2498 | Train Accuracy 0.2262 \n",
      "         Test accuracy 0.2100\n",
      "Epoch 00007 | Loss 2.2394 | Train Accuracy 0.2266 \n",
      "         Test accuracy 0.2113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, training_labels\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mlong))\n\u001b[0;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 11\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m acc \u001b[39m=\u001b[39m (logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m training_labels)\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m training_labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RGCN(num_nodes=g.num_nodes(), in_dim=256, h_dim=16, out_dim=dataset.num_classes, num_rels=num_rels, num_bases=40, reg='basis')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    logits = model(g)[training_idx].squeeze(1)\n",
    "    loss = F.cross_entropy(logits, training_labels.to(torch.long))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = (logits.argmax(dim=1) == training_labels).sum() / training_labels.size(0)\n",
    "    print(f'Epoch {epoch:05d} | Loss {loss.item():.4f} | Train Accuracy {acc:.4f} ')\n",
    "\n",
    "    acc = evaluate(g, withheld_idx, withheld_labels, model)\n",
    "    print(\"         Test accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import TransR,TransE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = TransE(num_rels=len(data.i2r), feats=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 3 required positional arguments: 'h_head', 'h_tail', and 'rels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scorer\u001b[39m.\u001b[39;49mforward()\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 3 required positional arguments: 'h_head', 'h_tail', and 'rels'"
     ]
    }
   ],
   "source": [
    "scorer.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphembedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgraphembedding\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgraphembedding\u001b[39;00m \u001b[39mimport\u001b[39;00m TransE\n\u001b[0;32m      3\u001b[0m a \u001b[39m=\u001b[39m TransE(data\u001b[39m.\u001b[39mtriples)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphembedding'"
     ]
    }
   ],
   "source": [
    "import graphembedding\n",
    "from graphembedding import TransE\n",
    "a = TransE(data.triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransE' from 'graphembedding' (c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgraphembedding\u001b[39;00m \u001b[39mimport\u001b[39;00m TransE\n\u001b[0;32m      2\u001b[0m a \u001b[39m=\u001b[39m TransE(data\u001b[39m.\u001b[39mtriples)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TransE' from 'graphembedding' (c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from graphembedding import TransE\n",
    "a = TransE(data.triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphembedding import complEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "185557",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[39m=\u001b[39m complEx(data\u001b[39m.\u001b[39;49mtriples)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\embedding.py:117\u001b[0m, in \u001b[0;36mcomplEx\u001b[1;34m(triplets, embed_size, n3_reg, learning_rate, num_negs, batch_size, num_epochs, callbacks, keras_model, return_keras_model, verbose)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomplEx\u001b[39m(triplets:np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m     84\u001b[0m             embed_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[0;32m     85\u001b[0m             n3_reg\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m             return_keras_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m             verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     94\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39m    Node & Edge Embedding using complEx Algorithm.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m        - [model] : 학습에 이용된 tf.keras.Model\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     dataset \u001b[39m=\u001b[39m ComplExDataset(triplets)\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m keras_model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         model \u001b[39m=\u001b[39m create_complExModel(\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mnodes), \u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39medges),\n\u001b[0;32m    121\u001b[0m                                     embed_size, n3_reg, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\dataset.py:74\u001b[0m, in \u001b[0;36mComplExDataset.__init__\u001b[1;34m(self, triplets)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes \u001b[39m=\u001b[39m get_nodes_from_triplets(triplets)\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medges \u001b[39m=\u001b[39m get_edges_from_triplets(triplets)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_triplets \u001b[39m=\u001b[39m encode_triplets(triplets, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnodes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medges)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\dataset.py:198\u001b[0m, in \u001b[0;36mencode_triplets\u001b[1;34m(triplets, nodes, edges)\u001b[0m\n\u001b[0;32m    195\u001b[0m node2id \u001b[39m=\u001b[39m {node: i \u001b[39mfor\u001b[39;00m i, node \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(nodes)}\n\u001b[0;32m    196\u001b[0m edge2id \u001b[39m=\u001b[39m {edge: i \u001b[39mfor\u001b[39;00m i, edge \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(edges)}\n\u001b[1;32m--> 198\u001b[0m subs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvectorize(node2id\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m)(triplets[:, \u001b[39m0\u001b[39;49m])\n\u001b[0;32m    199\u001b[0m rels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(edge2id\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m)(triplets[:, \u001b[39m1\u001b[39m])\n\u001b[0;32m    200\u001b[0m objs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(node2id\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m)(triplets[:, \u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\numpy\\lib\\function_base.py:2329\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     vargs \u001b[39m=\u001b[39m [args[_i] \u001b[39mfor\u001b[39;00m _i \u001b[39min\u001b[39;00m inds]\n\u001b[0;32m   2327\u001b[0m     vargs\u001b[39m.\u001b[39mextend([kwargs[_n] \u001b[39mfor\u001b[39;00m _n \u001b[39min\u001b[39;00m names])\n\u001b[1;32m-> 2329\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vectorize_call(func\u001b[39m=\u001b[39;49mfunc, args\u001b[39m=\u001b[39;49mvargs)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\numpy\\lib\\function_base.py:2407\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2405\u001b[0m     res \u001b[39m=\u001b[39m func()\n\u001b[0;32m   2406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2407\u001b[0m     ufunc, otypes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_ufunc_and_otypes(func\u001b[39m=\u001b[39;49mfunc, args\u001b[39m=\u001b[39;49margs)\n\u001b[0;32m   2409\u001b[0m     \u001b[39m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m     inputs \u001b[39m=\u001b[39m [asanyarray(a, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\numpy\\lib\\function_base.py:2367\u001b[0m, in \u001b[0;36mvectorize._get_ufunc_and_otypes\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcannot call `vectorize` on size 0 inputs \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2364\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39munless `otypes` is set\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2366\u001b[0m inputs \u001b[39m=\u001b[39m [arg\u001b[39m.\u001b[39mflat[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[1;32m-> 2367\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   2369\u001b[0m \u001b[39m# Performance note: profiling indicates that -- for simple\u001b[39;00m\n\u001b[0;32m   2370\u001b[0m \u001b[39m# functions at least -- this wrapping can almost double the\u001b[39;00m\n\u001b[0;32m   2371\u001b[0m \u001b[39m# execution time.\u001b[39;00m\n\u001b[0;32m   2372\u001b[0m \u001b[39m# Hence we make it optional.\u001b[39;00m\n\u001b[0;32m   2373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache:\n",
      "\u001b[1;31mKeyError\u001b[0m: 185557"
     ]
    }
   ],
   "source": [
    "a = complEx(data.triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/github-playground/playground.h5\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://storage.googleapis.com/github-playground/playground.h5: 403 -- Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\keras-2.11.0-py3.8.egg\\keras\\utils\\data_utils.py:300\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 300\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    301\u001b[0m \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\keras-2.11.0-py3.8.egg\\keras\\utils\\data_utils.py:84\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m response \u001b[39m=\u001b[39m urlopen(url, data)\n\u001b[0;32m     85\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fd:\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\urllib\\request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 222\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\urllib\\request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    530\u001b[0m     meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 531\u001b[0m     response \u001b[39m=\u001b[39m meth(req, response)\n\u001b[0;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\urllib\\request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m):\n\u001b[1;32m--> 640\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49merror(\n\u001b[0;32m    641\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m'\u001b[39;49m, request, response, code, msg, hdrs)\n\u001b[0;32m    643\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\urllib\\request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    568\u001b[0m args \u001b[39m=\u001b[39m (\u001b[39mdict\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp_error_default\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m orig_args\n\u001b[1;32m--> 569\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\urllib\\request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 502\u001b[0m result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\04_RESSOURCES\\python_3_6\\lib\\urllib\\request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_error_default\u001b[39m(\u001b[39mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 649\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(req\u001b[39m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgraphembedding\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplayground\u001b[39;00m \u001b[39mimport\u001b[39;00m load_github\n\u001b[1;32m----> 2\u001b[0m github_dataset \u001b[39m=\u001b[39m load_github() \n\u001b[0;32m      3\u001b[0m triplets \u001b[39m=\u001b[39m github_dataset[[\u001b[39m'\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrelation\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\playground\\github.py:21\u001b[0m, in \u001b[0;36mload_github\u001b[1;34m(name, event_types)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"knowledge graph Dataset을 불러오는 함수\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m현재 3가지 github knowledge graph가 구성되어 있음\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mname : (linux, tensorflow, vim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m               PushEvent, ReleaseEvent, WatchEvent)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_file\n\u001b[1;32m---> 21\u001b[0m fpath \u001b[39m=\u001b[39m get_file(\u001b[39m\"\u001b[39;49m\u001b[39mgithub-playground.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     22\u001b[0m                  \u001b[39m\"\u001b[39;49m\u001b[39mhttps://storage.googleapis.com/github-playground/playground.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     23\u001b[0m target_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_hdf(fpath, key\u001b[39m=\u001b[39mname)\n\u001b[0;32m     25\u001b[0m type_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_hdf(fpath, key\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\keras-2.11.0-py3.8.egg\\keras\\utils\\data_utils.py:302\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    300\u001b[0m     urlretrieve(origin, fpath, DLProgbar())\n\u001b[0;32m    301\u001b[0m \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mHTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 302\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39mcode, e\u001b[39m.\u001b[39mmsg))\n\u001b[0;32m    303\u001b[0m \u001b[39mexcept\u001b[39;00m urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_msg\u001b[39m.\u001b[39mformat(origin, e\u001b[39m.\u001b[39merrno, e\u001b[39m.\u001b[39mreason))\n",
      "\u001b[1;31mException\u001b[0m: URL fetch failure on https://storage.googleapis.com/github-playground/playground.h5: 403 -- Forbidden"
     ]
    }
   ],
   "source": [
    "from graphembedding.playground import load_github\n",
    "github_dataset = load_github() \n",
    "triplets = github_dataset[['subject','relation','object']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralkg\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "import wandb\n",
    "from neuralkg.utils import setup_parser\n",
    "from neuralkg.utils.tools import *\n",
    "from neuralkg.data.Sampler import *\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataload',\n",
       " 'embed',\n",
       " 'evaluate',\n",
       " 'main.py',\n",
       " 'outlier-detection.ipynb',\n",
       " 'outputs',\n",
       " 'preprocess',\n",
       " 'process.py',\n",
       " 'test.png',\n",
       " 'test.py',\n",
       " 'train_model.py',\n",
       " 'transE.ipynb',\n",
       " 'typings',\n",
       " 'unit_test.ipynb',\n",
       " 'utils',\n",
       " '__init__.py']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 321\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "parser = setup_parser()\n",
    "args = parser.parse_args(args=[])\n",
    "#specify the config path\n",
    "config_path = \"../config/TransE_demo_kg.yaml\"\n",
    "\n",
    "#load config to args\n",
    "args = load_config(args, config_path)\n",
    "args.data_path = \"../dataset/FB15K237\"\n",
    "#set random seeds\n",
    "seed_everything(args.seed)\n",
    "#set up sampler,datamodule,model,litmodel dynamicly\n",
    "train_sampler = import_class(f\"neuralkg.data.{args.train_sampler_class}\")(args)\n",
    "test_sampler = import_class(f\"neuralkg.data.{args.test_sampler_class}\")(train_sampler)\n",
    "kgdata = import_class(f\"neuralkg.data.{args.data_class}\")(args, train_sampler, test_sampler)\n",
    "model = import_class(f\"neuralkg.model.{args.model_name}\")(args)\n",
    "lit_model = import_class(f\"neuralkg.lit_model.{args.litmodel_name}\")(model, args)\n",
    "#set up logger, TensorBoardLogger is used by default\n",
    "logger = pl.loggers.TensorBoardLogger(\"training/logs\")\n",
    "if args.use_wandb:\n",
    "    log_name = \"_\".join([args.model_name, args.dataset_name, str(args.lr)])\n",
    "    logger = pl.loggers.WandbLogger(name=log_name, project=\"NeuralKG\")\n",
    "    logger.log_hyperparams(vars(args))\n",
    "#set up early_callback to early stopping\n",
    "early_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor=\"Eval|mrr\",\n",
    "        mode=\"max\",\n",
    "        patience=args.early_stop_patience,\n",
    "        check_on_train_epoch_end=False,\n",
    "    )\n",
    "#set up model saving method\n",
    "dirpath = \"/\".join([\"output\", args.eval_task, args.dataset_name, args.model_name])\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"Eval|mrr\",\n",
    "    mode=\"max\",\n",
    "    filename=\"{epoch}-{Eval|mrr:.3f}\",\n",
    "    dirpath=dirpath,\n",
    "    save_weights_only=True,\n",
    "    save_top_k=1,\n",
    ")\n",
    "callbacks = [early_callback, model_checkpoint]\n",
    "# initialize trainer\n",
    "trainer = pl.Trainer.from_argparse_args(\n",
    "        args,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        default_root_dir=\"training/logs\",\n",
    "        gpus=\"0,\",\n",
    "        check_val_every_n_epoch=args.check_per_epoch,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Missing logger folder: training/logs\\default\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | TransE   | 443 K \n",
      "1 | loss  | Adv_Loss | 443 K \n",
      "-----------------------------------\n",
      "443 K     Trainable params\n",
      "2         Non-trainable params\n",
      "443 K     Total params\n",
      "1.773     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 10/54423 [00:52<79:29:55,  5.26s/it, loss=0.856, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 71958/71958 [26:15<00:00, 45.68it/s, loss=0.714, v_num=0, Eval|mrr=0.007, Eval|hits@10=0.009]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'E:/00_CODE/03_Master_Thesis/rdf-literal-preprocessing/src/output/link_prediction/demo_kg/TransE/epoch=1-Eval|mrr=0.007.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#train&eval model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lit_model, datamodule\u001b[39m=\u001b[39;49mkgdata)\n\u001b[0;32m      3\u001b[0m \u001b[39m#test model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m lit_model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[0;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[0;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m     )\n\u001b[0;32m    739\u001b[0m     train_dataloaders \u001b[39m=\u001b[39m train_dataloader\n\u001b[1;32m--> 740\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    741\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    742\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[39mas all errors should funnel through them\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    686\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[39m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[0;32m    776\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m--> 777\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    779\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m   1198\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[1;32m-> 1199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[0;32m   1201\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mrun_stage()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:145\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:234\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m data_fetcher \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mget_profiled_dataloader(dataloader)\n\u001b[0;32m    233\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(data_fetcher)\n\u001b[0;32m    236\u001b[0m     \u001b[39m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[39m# as they expect that the same step is used when logging epoch end metrics even when the batch loop has\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[39m# finished. this means the attribute does not exactly track the number of optimizer steps applied.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# TODO(@carmocca): deprecate and rename so users don't get confused\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:146\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:242\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[0;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_validation()\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m# -----------------------------------------\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39m# SAVE LOGGERS (ie: Tensorboard, etc...)\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m# -----------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:337\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loop\u001b[39m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[0;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:151\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_end()\n\u001b[0;32m    152\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:140\u001b[0m, in \u001b[0;36mEvaluationLoop.on_run_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m eval_loop_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger_connector\u001b[39m.\u001b[39mupdate_eval_epoch_metrics()\n\u001b[0;32m    139\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_evaluation_end()\n\u001b[0;32m    142\u001b[0m \u001b[39m# enable train mode again\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_evaluation_model_train()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:202\u001b[0m, in \u001b[0;36mEvaluationLoop._on_evaluation_end\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mcall_hook(\u001b[39m\"\u001b[39m\u001b[39mon_test_end\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcall_hook(\u001b[39m\"\u001b[39;49m\u001b[39mon_validation_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    204\u001b[0m \u001b[39m# reset the logger connector state\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlogger_connector\u001b[39m.\u001b[39mreset_results()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1495\u001b[0m, in \u001b[0;36mTrainer.call_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m callback_fx \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, hook_name, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1494\u001b[0m \u001b[39mif\u001b[39;00m callable(callback_fx):\n\u001b[1;32m-> 1495\u001b[0m     callback_fx(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1497\u001b[0m \u001b[39m# next call hook in lightningModule\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\callback_hook.py:221\u001b[0m, in \u001b[0;36mTrainerCallbackHookMixin.on_validation_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called when the validation loop ends.\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m--> 221\u001b[0m     callback\u001b[39m.\u001b[39;49mon_validation_end(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:333\u001b[0m, in \u001b[0;36mModelCheckpoint.on_validation_end\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_skip_saving_checkpoint(trainer)\n\u001b[0;32m    328\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_on_train_epoch_end\n\u001b[0;32m    329\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    330\u001b[0m     \u001b[39mor\u001b[39;00m (trainer\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    331\u001b[0m ):\n\u001b[0;32m    332\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_checkpoint(trainer)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:396\u001b[0m, in \u001b[0;36mModelCheckpoint.save_checkpoint\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    391\u001b[0m monitor_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_candidates(trainer, epoch\u001b[39m=\u001b[39mepoch, step\u001b[39m=\u001b[39mglobal_step)\n\u001b[0;32m    393\u001b[0m \u001b[39m# callback supports multiple simultaneous modes\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[39m# here we call each mode sequentially\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[39m# Mode 1: save the top k checkpoints\u001b[39;00m\n\u001b[1;32m--> 396\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_top_k_checkpoint(trainer, monitor_candidates)\n\u001b[0;32m    397\u001b[0m \u001b[39m# Mode 2: save monitor=None checkpoints\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_none_monitor_checkpoint(trainer, monitor_candidates)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:683\u001b[0m, in \u001b[0;36mModelCheckpoint._save_top_k_checkpoint\u001b[1;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[0;32m    680\u001b[0m current \u001b[39m=\u001b[39m monitor_candidates\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor)\n\u001b[0;32m    682\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_monitor_top_k(trainer, current):\n\u001b[1;32m--> 683\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_best_and_save(current, trainer, monitor_candidates)\n\u001b[0;32m    684\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[0;32m    685\u001b[0m     epoch \u001b[39m=\u001b[39m monitor_candidates\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:740\u001b[0m, in \u001b[0;36mModelCheckpoint._update_best_and_save\u001b[1;34m(self, current, trainer, monitor_candidates)\u001b[0m\n\u001b[0;32m    735\u001b[0m     step \u001b[39m=\u001b[39m monitor_candidates\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    736\u001b[0m     rank_zero_info(\n\u001b[0;32m    737\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, global step \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor\u001b[39m}\u001b[39;00m\u001b[39m reached \u001b[39m\u001b[39m{\u001b[39;00mcurrent\u001b[39m:\u001b[39;00m\u001b[39m0.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    738\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (best \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_score\u001b[39m:\u001b[39;00m\u001b[39m0.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m), saving model to \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m as top \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    739\u001b[0m     )\n\u001b[1;32m--> 740\u001b[0m trainer\u001b[39m.\u001b[39;49msave_checkpoint(filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_weights_only)\n\u001b[0;32m    742\u001b[0m \u001b[39mif\u001b[39;00m del_filepath \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filepath \u001b[39m!=\u001b[39m del_filepath:\n\u001b[0;32m    743\u001b[0m     trainer\u001b[39m.\u001b[39mtraining_type_plugin\u001b[39m.\u001b[39mremove_checkpoint(del_filepath)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1913\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[1;34m(self, filepath, weights_only)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, filepath: _PATH, weights_only: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1913\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_connector\u001b[39m.\u001b[39;49msave_checkpoint(filepath, weights_only)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:478\u001b[0m, in \u001b[0;36mCheckpointConnector.save_checkpoint\u001b[1;34m(self, filepath, weights_only)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \n\u001b[0;32m    473\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m    filepath: write-target file's path\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m    weights_only: saving model weights only\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    477\u001b[0m _checkpoint \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[1;32m--> 478\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49msave_checkpoint(_checkpoint, filepath)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py:294\u001b[0m, in \u001b[0;36mTrainingTypePlugin.save_checkpoint\u001b[1;34m(self, checkpoint, filepath)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    checkpoint: dict containing model and trainer state\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    filepath: write-target file's path\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_rank_save_checkpoint:\n\u001b[1;32m--> 294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_io\u001b[39m.\u001b[39;49msave_checkpoint(checkpoint, filepath)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\plugins\\io\\torch_plugin.py:37\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[1;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[0;32m     34\u001b[0m fs\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39m# write the checkpoint dictionary on the file\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     atomic_save(checkpoint, path)\n\u001b[0;32m     38\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     39\u001b[0m     \u001b[39m# todo (sean): is this try catch necessary still?\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[39m# https://github.com/PyTorchLightning/pytorch-lightning/pull/431\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     key \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mLightningModule\u001b[39m.\u001b[39mCHECKPOINT_HYPER_PARAMS_KEY\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\pytorch_lightning\\utilities\\cloud_io.py:69\u001b[0m, in \u001b[0;36matomic_save\u001b[1;34m(checkpoint, filepath)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     torch\u001b[39m.\u001b[39msave(checkpoint, bytesbuffer)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mwith\u001b[39;00m fsspec\u001b[39m.\u001b[39mopen(filepath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     70\u001b[0m     f\u001b[39m.\u001b[39mwrite(bytesbuffer\u001b[39m.\u001b[39mgetvalue())\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\fsspec\\core.py:103\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    101\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49mmode)\n\u001b[0;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfobjects \u001b[39m=\u001b[39m [f]\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\fsspec\\spec.py:1106\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     ac \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mautocommit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_intrans)\n\u001b[1;32m-> 1106\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(\n\u001b[0;32m   1107\u001b[0m         path,\n\u001b[0;32m   1108\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   1109\u001b[0m         block_size\u001b[39m=\u001b[39;49mblock_size,\n\u001b[0;32m   1110\u001b[0m         autocommit\u001b[39m=\u001b[39;49mac,\n\u001b[0;32m   1111\u001b[0m         cache_options\u001b[39m=\u001b[39;49mcache_options,\n\u001b[0;32m   1112\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1113\u001b[0m     )\n\u001b[0;32m   1114\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1115\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mfsspec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompression\u001b[39;00m \u001b[39mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\fsspec\\implementations\\local.py:175\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_mkdir \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 175\u001b[0m \u001b[39mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\fsspec\\implementations\\local.py:273\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression \u001b[39m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 273\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open()\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\fsspec\\implementations\\local.py:278\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf\u001b[39m.\u001b[39mclosed:\n\u001b[0;32m    277\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocommit \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode:\n\u001b[1;32m--> 278\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    279\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression:\n\u001b[0;32m    280\u001b[0m             compress \u001b[39m=\u001b[39m compr[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompression]\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'E:/00_CODE/03_Master_Thesis/rdf-literal-preprocessing/src/output/link_prediction/demo_kg/TransE/epoch=1-Eval|mrr=0.007.ckpt'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 71958/71958 [26:23<00:00, 45.44it/s, loss=0.714, v_num=0, Eval|mrr=0.007, Eval|hits@10=0.009]"
     ]
    }
   ],
   "source": [
    "#train&eval model\n",
    "trainer.fit(lit_model, datamodule=kgdata)\n",
    "#test model\n",
    "lit_model.eval()\n",
    "trainer.test(lit_model, datamodule=kgdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "        super(TransE, self).__init__()\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "    def forward(self, head, relation, tail):\n",
    "        head_embedding = self.entity_embeddings(head)\n",
    "        relation_embedding = self.relation_embeddings(relation)\n",
    "        tail_embedding = self.entity_embeddings(tail)\n",
    "        score = torch.norm(head_embedding + relation_embedding - tail_embedding, p=1, dim=1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransE(data.num_entities,data.num_relations,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.8390, 0.7453, 0.4912,  ..., 0.6131, 0.4018, 1.4004],\n",
       "       grad_fn=<NormBackward1>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data.triples[:,0],data.triples[:,1],data.triples[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(176413, dtype=torch.int32) tensor(131, dtype=torch.int32) tensor(120236, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i, (head, relation, tail) in enumerate(data.triples):\n",
    "    print(head, relation, tail)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_sample(data,head,relation):\n",
    "    df = data.triples\n",
    "    ents = torch.arange(data.num_entities)\n",
    "    sub = df[df[:,0] == head ]\n",
    "    #print(sub)\n",
    "    sub = sub[sub[:,1] == relation ]\n",
    "    print(sub[:,2])\n",
    "    #print(df[df[:,0] == head ][:,2])\n",
    "\n",
    "    positive_ents = torch.unique(sub[:,2])\n",
    "    print(len(ents))\n",
    "    for e in positive_ents:\n",
    "        ents = torch.cat((ents[:e], ents[e+1:]))\n",
    "        #ents=ents[ents[~e]]\n",
    "    print(len(ents))\n",
    "\n",
    "    random_index = torch.randint(0, len(ents), (1,))\n",
    "\n",
    "    # Access the random element in the tensor\n",
    "    return ents[random_index]\n",
    "    \n",
    "    #df[df[:,0]== head and df[:,0]== relation][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(120236, dtype=torch.int32)\n",
      "tensor([120236], dtype=torch.int32)\n",
      "331089\n",
      "331088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([240154])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.triples[0,2])\n",
    "get_neg_sample(data,data.triples[0,0],data.triples[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m num_entities \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mnum_entities\n\u001b[0;32m      2\u001b[0m num_relations \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mnum_relations\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m TransE(num_entities, num_relations, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "num_entities = data.num_entities\n",
    "num_relations = data.num_relations\n",
    "model = TransE(num_entities, num_relations, 1)\n",
    "model(data.triples[:2,0],data.triples[:2,1],data.triples[:2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m \u001b[39m# Generate negative samples\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#negative_tail = torch.randint(num_entities, size=(len(head),))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m positive_score \u001b[39m=\u001b[39m model(head, relation, tail)\n\u001b[0;32m     19\u001b[0m negative_score \u001b[39m=\u001b[39m model(head, relation, get_neg_sample(data,head,relation))\n\u001b[0;32m     20\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m, in \u001b[0;36mTransE.forward\u001b[1;34m(self, head, relation, tail)\u001b[0m\n\u001b[0;32m     11\u001b[0m relation_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelation_embeddings(relation)\n\u001b[0;32m     12\u001b[0m tail_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentity_embeddings(tail)\n\u001b[1;32m---> 13\u001b[0m score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnorm(head_embedding \u001b[39m+\u001b[39;49m relation_embedding \u001b[39m-\u001b[39;49m tail_embedding, p\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\torch\\functional.py:1529\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1528\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1529\u001b[0m         \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mnorm(\u001b[39minput\u001b[39;49m, p, _dim, keepdim\u001b[39m=\u001b[39;49mkeepdim)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1531\u001b[0m         \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mnorm(\u001b[39minput\u001b[39m, p, _dim, keepdim\u001b[39m=\u001b[39mkeepdim, dtype\u001b[39m=\u001b[39mdtype)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "num_entities = data.num_entities\n",
    "num_relations = data.num_relations\n",
    "embedding_dim = 1\n",
    "num_epochs = 20\n",
    "# Define model, optimizer, and loss function\n",
    "model = TransE(num_entities, num_relations, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MarginRankingLoss(margin=1.0)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (head, relation, tail) in enumerate(data.triples):\n",
    "        optimizer.zero_grad()\n",
    "        # Generate negative samples\n",
    "        #negative_tail = torch.randint(num_entities, size=(len(head),))\n",
    "        # Forward pass\n",
    "        positive_score = model(head, relation, tail)\n",
    "        negative_score = model(head, relation, get_neg_sample(data,head,relation))\n",
    "        # Compute loss\n",
    "        loss = loss_fn(positive_score, negative_score, torch.ones(len(head)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {}/{}, Loss: {:.4f}\".format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphembedding import complEx, transE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "176413",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transE(data\u001b[39m.\u001b[39;49mtriples)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\embedding.py:59\u001b[0m, in \u001b[0;36mtransE\u001b[1;34m(triplets, embed_size, ord, margin, learning_rate, batch_size, num_epochs, callbacks, keras_model, return_keras_model, verbose)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransE\u001b[39m(triplets:np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m     27\u001b[0m            embed_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[0;32m     28\u001b[0m            \u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m            return_keras_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     36\u001b[0m            verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     37\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m    Node & Edge Embedding using transE Algorithm.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m        - [model] : 학습에 이용된 tf.keras.Model\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     dataset \u001b[39m=\u001b[39m TransEDataset(triplets)\n\u001b[0;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m keras_model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         model \u001b[39m=\u001b[39m create_transEModel(\u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39mnodes), \u001b[39mlen\u001b[39m(dataset\u001b[39m.\u001b[39medges),\n\u001b[0;32m     63\u001b[0m                                    embed_size, \u001b[39mord\u001b[39m, margin, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\dataset.py:27\u001b[0m, in \u001b[0;36mTransEDataset.__init__\u001b[1;34m(self, triplets)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnodes \u001b[39m=\u001b[39m get_nodes_from_triplets(triplets)\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medges \u001b[39m=\u001b[39m get_edges_from_triplets(triplets)\n\u001b[1;32m---> 27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_triplets \u001b[39m=\u001b[39m encode_triplets(triplets, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnodes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medges)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\graphembedding-0.1-py3.8.egg\\graphembedding\\dataset.py:198\u001b[0m, in \u001b[0;36mencode_triplets\u001b[1;34m(triplets, nodes, edges)\u001b[0m\n\u001b[0;32m    195\u001b[0m node2id \u001b[39m=\u001b[39m {node: i \u001b[39mfor\u001b[39;00m i, node \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(nodes)}\n\u001b[0;32m    196\u001b[0m edge2id \u001b[39m=\u001b[39m {edge: i \u001b[39mfor\u001b[39;00m i, edge \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(edges)}\n\u001b[1;32m--> 198\u001b[0m subs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvectorize(node2id\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m)(triplets[:, \u001b[39m0\u001b[39;49m])\n\u001b[0;32m    199\u001b[0m rels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(edge2id\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m)(triplets[:, \u001b[39m1\u001b[39m])\n\u001b[0;32m    200\u001b[0m objs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(node2id\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m)(triplets[:, \u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\numpy\\lib\\function_base.py:2329\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     vargs \u001b[39m=\u001b[39m [args[_i] \u001b[39mfor\u001b[39;00m _i \u001b[39min\u001b[39;00m inds]\n\u001b[0;32m   2327\u001b[0m     vargs\u001b[39m.\u001b[39mextend([kwargs[_n] \u001b[39mfor\u001b[39;00m _n \u001b[39min\u001b[39;00m names])\n\u001b[1;32m-> 2329\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vectorize_call(func\u001b[39m=\u001b[39;49mfunc, args\u001b[39m=\u001b[39;49mvargs)\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\numpy\\lib\\function_base.py:2407\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2405\u001b[0m     res \u001b[39m=\u001b[39m func()\n\u001b[0;32m   2406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2407\u001b[0m     ufunc, otypes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_ufunc_and_otypes(func\u001b[39m=\u001b[39;49mfunc, args\u001b[39m=\u001b[39;49margs)\n\u001b[0;32m   2409\u001b[0m     \u001b[39m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m     inputs \u001b[39m=\u001b[39m [asanyarray(a, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n",
      "File \u001b[1;32mc:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\lib\\site-packages\\numpy\\lib\\function_base.py:2367\u001b[0m, in \u001b[0;36mvectorize._get_ufunc_and_otypes\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcannot call `vectorize` on size 0 inputs \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2364\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39munless `otypes` is set\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2366\u001b[0m inputs \u001b[39m=\u001b[39m [arg\u001b[39m.\u001b[39mflat[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args]\n\u001b[1;32m-> 2367\u001b[0m outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   2369\u001b[0m \u001b[39m# Performance note: profiling indicates that -- for simple\u001b[39;00m\n\u001b[0;32m   2370\u001b[0m \u001b[39m# functions at least -- this wrapping can almost double the\u001b[39;00m\n\u001b[0;32m   2371\u001b[0m \u001b[39m# execution time.\u001b[39;00m\n\u001b[0;32m   2372\u001b[0m \u001b[39m# Hence we make it optional.\u001b[39;00m\n\u001b[0;32m   2373\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache:\n",
      "\u001b[1;31mKeyError\u001b[0m: 176413"
     ]
    }
   ],
   "source": [
    "transE(data.triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No random seed is specified. Setting to 1173960838.\n",
      "Training epochs on cuda:0:   0%|          | 0/5 [00:00<?, ?epoch/s]\n",
      "Training batches on cuda:0:   0%|          | 0/7 [00:00<?, ?batch/s]\n",
      "Training epochs on cuda:0:  20%|██        | 1/5 [00:02<00:09,  2.35s/epoch, loss=1.47, prev_loss=nan]\n",
      "Training batches on cuda:0:   0%|          | 0/7 [00:00<?, ?batch/s]\n",
      "Training epochs on cuda:0:  40%|████      | 2/5 [00:04<00:07,  2.37s/epoch, loss=1.43, prev_loss=1.47]\n",
      "Training batches on cuda:0:   0%|          | 0/7 [00:00<?, ?batch/s]\n",
      "Training epochs on cuda:0:  60%|██████    | 3/5 [00:07<00:04,  2.37s/epoch, loss=1.45, prev_loss=1.43]\n",
      "Training batches on cuda:0:   0%|          | 0/7 [00:00<?, ?batch/s]\n",
      "Training epochs on cuda:0:  80%|████████  | 4/5 [00:09<00:02,  2.36s/epoch, loss=1.39, prev_loss=1.45]\n",
      "Training batches on cuda:0:   0%|          | 0/7 [00:00<?, ?batch/s]\n",
      "Training epochs on cuda:0: 100%|██████████| 5/5 [00:11<00:00,  2.37s/epoch, loss=1.41, prev_loss=1.39]\n",
      "INFO:pykeen.evaluation.evaluator:Starting batch_size search for evaluation now...\n",
      "INFO:pykeen.evaluation.evaluator:Concluded batch_size search with batch_size=201.\n",
      "Evaluating on cuda:0: 100%|██████████| 201/201 [00:00<00:00, 4.06ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.07s seconds\n"
     ]
    }
   ],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "result = pipeline(\n",
    "    model='TransE',\n",
    "    dataset='nations',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineResult(random_seed=1173960838, model=TransE(\n",
       "  (loss): MarginRankingLoss(\n",
       "    (margin_activation): ReLU()\n",
       "  )\n",
       "  (interaction): TransEInteraction()\n",
       "  (entity_representations): ModuleList(\n",
       "    (0): Embedding(\n",
       "      (_embeddings): Embedding(14, 50)\n",
       "    )\n",
       "  )\n",
       "  (relation_representations): ModuleList(\n",
       "    (0): Embedding(\n",
       "      (_embeddings): Embedding(55, 50)\n",
       "    )\n",
       "  )\n",
       "  (weight_regularizers): ModuleList()\n",
       "), training=TriplesFactory(num_entities=14, num_relations=55, create_inverse_triples=False, num_triples=1592, path=\"C:\\Users\\Noctris\\.virtualenvs\\rdf-literal-preprocessing-20b3_M0v\\Lib\\site-packages\\pykeen\\datasets\\nations\\train.txt\"), training_loop=<pykeen.training.slcwa.SLCWATrainingLoop object at 0x00000184546FDDC0>, losses=[1.4652120215552193, 1.4338116134916032, 1.4517410823277064, 1.3901010070528304, 1.4133971759251185], metric_results=<pykeen.evaluation.rank_based_evaluator.RankBasedMetricResults object at 0x00000184546B1400>, train_seconds=17.051034927368164, evaluate_seconds=4.809956312179565, stopper=<pykeen.stoppers.stopper.NopStopper object at 0x0000018454750820>, configuration={'dataset': 'nations', 'dataset_kwargs': None, 'model': 'TransE', 'model_kwargs': {'random_seed': 1173960838, 'loss': MarginRankingLoss(\n",
       "  (margin_activation): ReLU()\n",
       ")}, 'loss_kwargs': None, 'regularizer_kwargs': None, 'optimizer': 'Adam', 'optimizer_kwargs': {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': False}, 'training_loop': 'SLCWATrainingLoop', 'training_loop_kwargs': {}, 'evaluator': 'RankBasedEvaluator', 'evaluator_kwargs': {}, 'num_epochs': 5, 'batch_size': 256, 'evaluation_kwargs': {'additional_filter_triples': {'training': {'sha512': '55186f67ee8112c8003519ea87c19bb6b9e110489e6b0a6abeb7a127f2dc9b7195a518f4829d773b82fd47632ac1fe09f6a256cef2469c7dc895210a10919bb4'}, 'validation': {'sha512': '171f020ddad54115ff23a5c7d7143aaf097472f5a76a47b55333ac7c5a522b045cf320b5c0e3842efe005bc017e3e3b1c412532a734195cb1102651babbf631f'}}}}, metadata={}, version='1.9.0', git_hash='UNHASHED')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdf-literal-preprocessing-20b3_M0v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fce7f7c728e806eecc03f5bf819d9415bfd370bb29c67dd1d9160ffa0efd741f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
